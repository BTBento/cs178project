{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a150bac-b7c6-4737-b503-2cea407fdbc6",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<h1> Classification Methods of the CIFAR-10 Dataset </h1>\n",
    "<h2> Alex Jen, Chan Young Ji, Everett Cheng </h2>\n",
    "<h3> CS 178: Machine Learning & Data Mining (Winter 2024)</h3>\n",
    "<h4> UCInetIDs: ajen2, jicy, everetc</h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8550f65f-9627-402c-873e-56590ad16c4e",
   "metadata": {},
   "source": [
    "### Table of Contents  \n",
    "\n",
    "1. ##### Summary\n",
    "2. ##### Data Description\n",
    "3. ##### Classifiers\n",
    "4. ##### Experimental Setup\n",
    "5. ##### Experimental Results\n",
    "6. ##### Insights\n",
    "7. ##### Contributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb47885-9326-4298-9343-3c36d577e50b",
   "metadata": {},
   "source": [
    "### Summary\n",
    "Summary a short 2-4 sentence summary of your project, identifying the dataset(s) you used, what classification methods you investigated, and 1 or 2 main conclusions from your investigations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf95e0bc-5c4b-4fa1-9762-1e5bdba0347e",
   "metadata": {},
   "source": [
    "### Data Description\n",
    "Describe the dataset(s) that you used in your project. Describe what dataset exploration you did, e.g., visualization, generation of summary statistics. Include at least 1 figure in this section. Try to find and read at least one published paper that has used this dataset for research in the past (you can use the paperswithcode Website to find papers). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd585da4-7ae0-458a-9ba7-92b818a2dd84",
   "metadata": {},
   "source": [
    "### Classifiers\n",
    "List the classification algorithms/methods that you used in your project. For each classification method provide a brief 1 or 2 sentence description of the method (e.g., imagine your report is being read by a computer scientist who is not familiar with the details of different classifiers, but would like a high-level summary of the characteristics of each). Mention what the hyperparameters are for each method (if any) and for each hyperparameter the range of values (or settings, or architectures) that you investigated in your experiments. Mention also what software you used for each method, e.g., scikit-learn or some other package like PyTorch (for more advanced neural networks)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "75187f9f-e36f-484b-9399-59b2fdec39ba",
   "metadata": {},
   "source": [
    "kNN, logistic, feedforward, and convolutional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9364c9da-ecc3-4bf0-b872-34d5fa8476b9",
   "metadata": {},
   "source": [
    "### Experimental Setup\n",
    "Describe briefly how you conducted your experimental methodology. What metrics did you look at? e..g, just classification accuracy (or error), or other metrics (such as precision/recall for binary classifiers). How did you partition up your data? we recommend that at the start of your project you set aside a test dataset that is only used once at the end of your project for final evaluation of models (this will be a realistic test of your method), e.g., 20% randomly selected examples, or the specified test set if the dataset has such.  The rest of the data (e.g., the remaining 80%) can be partitioned into a training and validation set (e.g., 75% of the 80% (i.e., 60% of the total) for training and 25% of the 80% (i.e, 20% of the total) for validation, where the validation set can be used for hyperparameter tuning. Be clear on how you selected hyparameters: feel free to use pseudocode for example to describe precisely what you did. In reporting machine learning experiments it is important that your experiments are reproducible (for others to independently validate and recreate what you did), and reproducibility implies clear documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44b41b1-95a3-468b-a77e-bea938bdb79a",
   "metadata": {},
   "source": [
    "### Experimental Results\n",
    "Summarize the results that you obtained on the test data for each of your classifiers and for each of the metrics that you used, and write a brief interpretation of the overall results. Provide the results as a table or as a figure (e.g., bar-chart). Feel free to add additional information, such as comparing performance of each model on test data to its performance on training data. Feel free to also additional experimental results (such how accuracy for certain classifiers varied if they were provided with less training data (i.e,. \"learning curves\"), or how accuracy varied as a function of regularization) - this level of detail can go in an Appendix if you don't have space in the main report. You could also analyze if classifiers make the same errors: e.g., if classifier A has an error rate of 5% and classifier B has an error rate of 10%, are all (or most) of classifier A's error included in classifier B's errors?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89273701-3257-4c83-8532-a7cc537273f7",
   "metadata": {},
   "source": [
    "### Insights\n",
    "Write a brief summary of what you learned by doing this project. For example did any of the results surprise you? You could also do an \"error analysis\" of the results by looking at the examples where the classifier made an error: are the examples that the classifier misclassifies also hard for humans to classify? and if so, what makes these examples difficult? Feel free to speculate on what you think the strengths and weaknesses of using any of the machine learning methods (that you investigated) on a more realistic version of the dataset(s) you worked with, e.g., putting the classifier on a real camera for object recognition, or using it in a real-world medical setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ead9eb-e156-4d25-a8dc-c83857a1d33f",
   "metadata": {},
   "source": [
    "### Contributions\n",
    "For each team member provide a 1 to 2 sentence description of how that team member contributed to the project (e.g., it could be in terms of contributions to the different aspects of your project as described in the section headings above). While there will inevitably be a fair bit of overlap in terms of what team members work on (and that is fine) please try to identify at least 1 or 2 aspects of the project, for each team member , where they did the most work relative to the rest of the team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1258d35-68d9-45bf-ba13-c4c1f8b1aafb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
